<!DOCTYPE html>

<html lang="en-us">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <!-- Font Awesome for social media icons -->
    <script src="https://kit.fontawesome.com/791291c78f.js" crossorigin="anonymous"></script>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <!-- Site Information -->
    <title> Rui Pan ÊΩòÁëû </title>

    <style type="text/css">
      .smlinks {
        color: black;
      }
      .smlinks:hover {
        color: rgb(7, 107, 255);
      }
      .paper-item {
        margin-bottom: 10px; /* Adjust this value to increase/decrease the space */
      }
    </style>

    <!-- Favicon -->
    <!-- https://realfavicongenerator.net/ -->
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">
    
    <!-- Google Search Console verification -->
    <meta name="google-site-verification" content="YguyxAOXZakXrwT46p_SOS-zOnrWxmoydLQ8moo4nn0" />
  </head>


  <body>
    <!-- Nav Bar -->
    <nav class="navbar navbar-expand-lg navbar-light sticky-top navbar-custom" style="background-color: #f58025">  <!-- f58025 or ff8f00 -->
      <a class="navbar-brand" href="#about">
        <img src="apple-touch-icon.png" width="30" height="30" class="d-inline-block align-top" alt="Rui's head">
        Rui Pan
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="#publications">Publications</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="#education">Education</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="#experience">Experience</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="#interests">Interests</a>
          </li>
          <!-- <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="https://blog.ruipan.xyz/" target="_blank">Blog</a>
          </li> -->
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="#contacts">Contacts</a>
          </li>
        </ul>
        <script>
          //GOOGLE SEARCH: Enter domain of site to search.
          var domainroot="ruipeterpan.github.io"
          function Gsitesearch(curobj){ curobj.q.value="site:"+domainroot+" "+curobj.qfront.value } 
        </script>
        <form class="form-inline my-2 my-lg-0" action="http://www.google.com/search" method="get" role="search" onSubmit="Gsitesearch(this)">
          <input name="q" type="hidden" />
          <input class="form-control mr-sm-2" name="qfront" type="search" placeholder="Search on this site" aria-label="Search">
        </form>
      </div>
    </nav>

    <!-- Jumbotron -->
    <div class="jumbotron jumbotron-fluid text-center">
      <div class="container">
        <div class="row align-items-center">
          <div class="col-sm-6">
            <!-- <img src='images/barcelona.jpg' alt="Rui's headshot" onmouseover="this.src='images/lightsaber.jpg';" onmouseout="this.src='images/barcelona.jpg';" style="border-radius: 50%;align-items: center;justify-content: center;overflow: hidden;margin-bottom:0.5cm;margin-top:0.5cm;" width="50%"> -->
            <img src='images/2026_headshot.jpg' alt="Rui's headshot" onmouseover="this.src='images/mii.jpeg';" onmouseout="this.src='images/2026_headshot.jpg';" style="border-radius: 50%;align-items: center;justify-content: center;overflow: hidden;margin-bottom:0.5cm;margin-top:0.5cm;" width="50%">
          </div>
          <div class="col-sm-4">
            <div>
              <h2 class="jumbotron-heading">Rui Pan ÊΩòÁëû</h2>
              <p class="lead text-muted">CS Ph.D. student @ Princeton</p>
              <a href="files/cv.pdf" target="_blank"><button type="button" class="btn btn-outline-primary">CV</button></a>
              <!-- <a href="https://blog.ruipan.xyz/" target="_blank"><button type="button" class="btn btn-outline-primary">Blog</button></a> -->
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- About -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
          <a id="about" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
          <h2>About</h2>
          <p>
            I'm a 4th-year CS Ph.D. student at <a href="https://www.cs.princeton.edu/" target="_blank">Princeton University</a>, advised by <a href="https://www.cs.princeton.edu/~ravian/" target="_blank">Prof. Ravi Netravali</a>.
            I am a member of the <a href="https://sysml.cs.princeton.edu/" target="_blank">Princeton Systems for AI Lab (SAIL)</a>.
            I am broadly interested in the intersection of systems, networks, and machine learning. My recent work has focused on systems and algorithms for efficient LLM inference.
            I got my B.S. in CS and Math from <a href="https://www.cs.wisc.edu/" target="_blank">University of Wisconsin-Madison</a>, where I was fortunate to work with <a href="https://shivaram.org/" target="_blank">Prof. Shivaram Venkataraman</a> on systems for distributed ML training.
            I have also interned at <a href="https://techsysinfra.google/research/" target="_blank">Google</a> and <a href="https://www.amazon.science/author/yida-wang" target="_blank">AWS</a> on efficient LLM inference and at <a href="https://www.mpi-inf.mpg.de/departments/network-and-cloud-systems" target="_blank">MPI-INF</a> on networks for ML training. 
            My research has been recognized with an <a href="https://mlsys.org/virtual/2025/poster/3260" target="_blank">MLSys Outstanding Paper Award</a> (Honorable Mention, 2025) and a <a href="https://www.janestreet.com/join-jane-street/programs-and-events/grf-profiles-2026" target="_blank">Jane Street Graduate Research Fellowship Finalist Award</a> (2026).
          </p>
        </div>
      </div>
    </div>

    <!-- Projects -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
          <a id="publications" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
          <div style="display: flex; align-items: center;">
            <h2 style="margin-right: 10px;">Publications</h2> (*Equal contributions)
          </div>
          <!--use btn-outline-secondary for unclickable, grey buttons-->
          <h4>Selected Publications</h4>
          <ul>
            <li class="paper-item">
              <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
              <!-- <a id="specreason" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a> -->
              <h5>Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</h5>
              <b>Rui Pan</b>, Zhuofu Chen, Hongyi Liu, Arvind Krishnamurthy, Ravi Netravali <br>
              arXiv 2025<br>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2512.20573" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#failfast-abstract" role="button" aria-expanded="false" aria-controls="failfast-abstract">Abstract</a>
                <a href="https://github.com/ruipeterpan/failfast" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Code</button></a>
              </div>
              <div class="collapse" id="failfast-abstract">
                <div class="card card-body">
                  Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff.
                  We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers.
                  Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding.
                  We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length.
                  It "fails fast" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and "wins big" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!).
                  Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9x speedup over vanilla decoding, 1.7x over the best naive dLLM drafter, and 1.4x over EAGLE-3 across diverse models and workloads.
                  We open-source FailFast at https://github.com/ruipeterpan/failfast.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
              <!-- For NeurIPS 2025 :) -->
              <a id="specreason" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
              <h5>SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning</h5>
              <b>Rui Pan</b>, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, Ravi Netravali <br>
              <a href="https://neurips.cc/Conferences/2025" target="_blank">NeurIPS 2025</a>, San Diego, CA<br>
              <span style="color: #e9a23b; font-weight: bold;">üèÜSpotlight Paper at the <a href="https://efficient-reasoning.github.io/" target="_blank" style="color: #e9a23b">NeurIPS 2025 Workshop on Efficient Reasoning</a></span><br>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2504.07891" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#specreason-abstract" role="button" aria-expanded="false" aria-controls="specreason-abstract">Abstract</a>
                <a href="publications/neurips25_specreason_poster.pdf" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Poster</button></a>
                <a href="https://github.com/ruipeterpan/specreason" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Code</button></a>
              </div>
              <div class="collapse" id="specreason-abstract">
                <div class="card card-body">
                  Recent advances in inference-time compute have significantly improved performance 
                  on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). 
                  However, this improved accuracy comes at the cost of high inference latency due to the 
                  length of generated reasoning sequences and the autoregressive nature of decoding. 
                  Our key insight in tackling these overheads is that LRM inference, and the reasoning 
                  that it embeds, is highly tolerant of approximations: complex tasks are typically broken 
                  down into simpler steps, each of which brings utility based on the semantic insight it 
                  provides for downstream steps rather than the exact tokens it generates. 
                  Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference 
                  by using a lightweight model to (speculatively) carry out simpler intermediate reasoning 
                  steps and reserving the costly base model only to efficiently assess (and potentially correct) 
                  the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic 
                  flexibility of thinking tokens in preserving final-answer accuracy is complementary 
                  to prior speculation techniques, most notably speculative decoding, which demands 
                  token-level equivalence at each step. Across a variety of cross-domain reasoning benchmarks, 
                  SpecReason achieves 1.4-3.0x speedup over vanilla LRM inference while improving 
                  accuracy by 0.4-9.0%. Compared to speculative decoding without SpecReason, their 
                  combination yields an additional 8.8-58.0% latency reduction. 
                  We open-source SpecReason at https://github.com/ruipeterpan/specreason.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>
                Marconi: Prefix Caching for the Era of Hybrid LLMs
                <img src="images/acm_available_1.1.png" height="25"/><img src="images/acm_functional_1.1.png" height="25"/>
              </h5> 
              <b>Rui Pan</b>, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali <br>
              <a href="https://mlsys.org/virtual/2025/index.html" target="_blank">MLSys 2025</a>, Santa Clara, CA<br>
              <span style="color: #e9a23b; font-weight: bold;">üèÜOutstanding Paper Award, Honorable Mention</span><br>
              <!-- <span style="color: #EA4335; font-weight: bold;">‚ù§Ô∏èRui's personal favorite</span><br> -->
              <span style="color: #C86534; font-weight: bold;"><img src="images/sglang.png" height="25"/>(Being) Integrated into SGLang!</span><br>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2411.19379" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#marconi-abstract" role="button" aria-expanded="false" aria-controls="marconi-abstract">Abstract</a>
                <a href="publications/mlsys25_marconi.key" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Slides</button></a>
                <a href="https://mlsys.org/virtual/2025/3260" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Talk</button></a>
                <a href="https://github.com/ruipeterpan/marconi" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Code</button></a>
                <a href="https://www.youtube.com/watch?v=K1b8AhIsSYQ" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">üéµ</button></a>
              </div>
              <div class="collapse" id="marconi-abstract">
                <div class="card card-body">
                  Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent
                  layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language
                  Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency
                  optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of
                  in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and
                  instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most
                  of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix
                  caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously
                  assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a
                  taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints.
                  Across diverse workloads and Hybrid models, Marconi achieves up to 34.4√ó higher token hit rates (71.1% or 617
                  ms lower TTFT) compared to state-of-the-art prefix caching systems.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>
                Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving
                <img src="images/acm_available_1.1.png" height="25"/><img src="images/acm_functional_1.1.png" height="25"/><img src="images/acm_reproduced_1.1.png" height="25"/>
              </h5> 
              Yinwei Dai*, <b>Rui Pan*</b>, Anand Iyer, Kai Li, Ravi Netravali <br>
              <a href="https://sigops.org/s/conferences/sosp/2024/" target="_blank">ACM SOSP 2024</a>, Austin, TX<br>
              <div class="mt-2">
                <a href="https://dl.acm.org/doi/10.1145/3694715.3695963?cid=99660641570" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#apparate-abstract" role="button" aria-expanded="false" aria-controls="apparate-abstract">Abstract</a>
                <a href="publications/sosp24_apparate_talk.key" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Slides</button></a>
                <a href="javascript:void(0);" target="_blank"><button type="button" class="btn btn-outline-secondary btn-sm">Talk</button></a>
                <a href="https://github.com/dywsjtu/apparate" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="apparate-abstract">
                <div class="card card-body">
                  Machine learning (ML) inference platforms are tasked with balancing two competing goals: 
                  ensuring high throughput given many requests, and delivering low-latency responses to support interactive applications. 
                  Unfortunately, existing platform knobs (e.g., batch sizes) fail to ease this fundamental tension, 
                  and instead only enable users to harshly trade off one property for the other. 
                  This paper explores an alternate strategy to taming throughput-latency tradeoffs by changing the granularity 
                  at which inference is performed. 
                  We present Apparate, a system that automatically applies and manages early exits (EEs) in ML models, 
                  whereby certain inputs can exit with results at intermediate layers. 
                  To cope with the time-varying overhead and accuracy challenges that EEs bring, 
                  Apparate repurposes exits to provide continual feedback that powers several novel runtime monitoring and adaptation strategies. 
                  Apparate lowers median response latencies by 40.5-91.5% and 10.0-24.2% for diverse CV and NLP classification workloads, 
                  and median time-per-token latencies by 70.4-77.9% for generative scenarios, 
                  without affecting throughputs or violating tight accuracy constraints.
                </div>
              </div>
            </li>
          </ul>
          <h4>All Publications</h4>
          <ul>
            <li class="paper-item">
              <h5>A benchmark of expert-level academic questions to assess AI capabilities</h5>
              Center for AI Safety, Scale AI, HLE Contributors Consortium <br>
              <a href="https://www.nature.com/nature/volumes/649/issues/8099" target="_blank">Nature 2026</a><br>
              <div class="mt-2">
                <a href="https://www.nature.com/articles/s41586-025-09962-4" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#hle-abstract" role="button" aria-expanded="false" aria-controls="hle-abstract">Abstract</a>
              </div>
              <div class="collapse" id="hle-abstract">
                <div class="card card-body">
                  Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. 
                  However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\% accuracy on popular benchmarks like MMLU, 
                  limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), 
                  a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark 
                  of its kind with broad subject coverage. HLE consists of 2,700 questions across dozens of subjects, including 
                  mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists 
                  of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that 
                  is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs 
                  demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and 
                  the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear 
                  understanding of model capabilities, we publicly release HLE at https://lastexam.ai/.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Optimizing Mixture-of-Experts Inference Time via Model Deployment and Communication Scheduling</h5>
              Jialong Li, Shreyansh Tripathi, Lakshay Rastogi, Yiming Lei, <b>Rui Pan</b>, Yiting Xia <br>
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10723154" target="_blank">IEEE ToN 2026</a><br>
              <div class="mt-2">
                <a href="publications/arxiv24_aurora.pdf" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#comet-abstract" role="button" aria-expanded="false" aria-controls="comet-abstract">Abstract</a>
                <!-- <a href="javascript:void(0);" target="_blank"><button type="button" class="btn btn-outline-secondary btn-sm">Code</button></a> -->
              </div>
              <div class="collapse" id="comet-abstract">
                <div class="card card-body">
                  As machine learning models scale in size and complexity, their computational requirements become a significant barrier. 
                  Mixture-of-Experts (MoE) models alleviate this issue by selectively activating relevant experts. 
                  Despite this, MoE models are hindered by high communication overhead from all-to-all operations, 
                  low GPU utilization, and complications from heterogeneous GPU environments.
                  This paper presents Comet, which optimizes both model deployment and all-to-all communication 
                  scheduling to address these challenges in MoE inference. 
                  Comet achieves minimal communication times by strategically ordering token transmissions 
                  in all-to-all communications. It improves GPU utilization by colocating experts 
                  from different models on the same device, avoiding the limitations of all-to-all communication. 
                  We analyze Comet's optimization strategies theoretically across four common GPU cluster settings: 
                  exclusive vs. colocated models on GPUs, and homogeneous vs. heterogeneous GPUs. 
                  Comet provides optimal solutions for three cases, and for the remaining NP-hard scenario, 
                  it offers a polynomial-time sub-optimal solution with only a 1.09√ó degradation from the optimal, 
                  as shown in the simulation results. Comet is the first approach to minimize MoE inference time via 
                  optimal model deployment and communication scheduling across various scenarios. 
                  Evaluations demonstrate that Comet significantly accelerates inference, 
                  achieving speedups of up to 2.63√ó in homogeneous clusters and 2.91√ó in heterogeneous environments. 
                  Moreover, Comet enhances GPU utilization by up to 2.38√ó compared to existing methods.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>METIS: Fast Quality-Aware RAG Systems with Configuration Adaptation</h5>
              Siddhant Ray, <b>Rui Pan</b>, Zhuohan Gu, Kuntai Du, Shaoting Feng, Ganesh Ananthanarayanan, Ravi Netravali, Junchen Jiang <br>
              <a href="https://sigops.org/s/conferences/sosp/2025/" target="_blank">ACM SOSP 2025</a>, Seoul, Korea<br>
              <div class="mt-2">
                <a href="https://dl.acm.org/doi/10.1145/3731569.3764855?cid=99660641570" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#metis-abstract" role="button" aria-expanded="false" aria-controls="metis-abstract">Abstract</a>
                <a href="publications/sosp25_metis_talk.pptx" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Slides</button></a>
                <!-- <a href="javascript:void(0);" target="_blank"><button type="button" class="btn btn-outline-secondary btn-sm">Code</button></a> -->
              </div>
              <div class="collapse" id="metis-abstract">
                <div class="card card-body">
                  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to 
                  generate better responses with external knowledge, but using more external 
                  knowledge often improves generation quality at the expense of response delay. 
                  Prior work either reduces the response delay (through better scheduling of RAG 
                  queries) or strives to maximize quality (which involves tuning the RAG workflow), 
                  but they fall short in optimizing the \emph {tradeoff} between the delay 
                  and quality of RAG responses. This paper presents METIS, the first RAG system 
                  that jointly schedules queries and adapts the key RAG configurations of each 
                  job, such as the number of retrieved text chunks and synthesis methods, 
                  in order to balance quality optimization and response delay reduction. 
                  Using 4 popular RAG-QA datasets, we show that compared with the state-of-the-art 
                  RAG scheduling system, METIS reduces the generation latency by 1.64--2.54√ó
                  without sacrificing generation quality.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Mowgli: Passively Learned Rate Control for Real-Time Video</h5> 
              Neil Agarwal, <b>Rui Pan</b>, Francis Y. Yan, Ravi Netravali <br>
              <a href="https://www.usenix.org/conference/nsdi25" target="_blank">USENIX NSDI 2025</a>, Philadelphia, PA<br>
              <div class="mt-2">
                <a href="https://www.usenix.org/system/files/nsdi25-agarwal.pdf" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#mowgli-abstract" role="button" aria-expanded="false" aria-controls="mowgli-abstract">Abstract</a>
                <a href="javascript:void(0);" target="_blank"><button type="button" class="btn btn-outline-secondary btn-sm">Slides</button></a>
                <a href="https://youtu.be/cZLwvoCfo0I" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Talk</button>
                </a>
              </div>
              <div class="collapse" id="mowgli-abstract">
                <div class="card card-body">
                  Rate control algorithms are at the heart of video conferencing platforms, 
                  determining target bitrates that match dynamic network characteristics for high quality. 
                  Recent data-driven strategies have shown promise for this challenging task, 
                  but the performance degradation they introduce during training has been a nonstarter 
                  for many production services, precluding adoption. 
                  This paper aims to bolster the practicality of data-driven rate control by presenting 
                  an alternative avenue for experiential learning: 
                  leveraging purely existing telemetry logs produced by the incumbent algorithm in production. 
                  We observe that these logs often contain effective decisions, although often at the wrong times or in the wrong order. 
                  To realize this approach despite the inherent uncertainty that log-based learning brings 
                  (i.e., lack of feedback for new decisions), our system, Mowgli, 
                  combines a variety of robust learning techniques (i.e., conservatively reasoning 
                  about alternate behavior to minimize risk and using a richer model formulation to account for environmental noise). 
                  Across diverse networks (emulated and real-world), Mowgli outperforms the widely deployed GCC algorithm, 
                  increasing average video bitrates by 15-39% while reducing freeze rates by 60-100%.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Improving DNN Inference Throughput Using Practical, Per-Input Compute Adaptation</h5> 
              Anand Iyer, Mingyu Guan, Yinwei Dai, <b>Rui Pan</b>, Swapnil Gandhi, Ravi Netravali <br>
              <a href="https://sigops.org/s/conferences/sosp/2024/" target="_blank">ACM SOSP 2024</a>, Austin, TX<br>
              <div class="mt-2">
                <a href=" https://dl.acm.org/doi/10.1145/3694715.3695978?cid=99660641570" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#e3-abstract" role="button" aria-expanded="false" aria-controls="e3-abstract">Abstract</a>
                <a href="publications/sosp24_e3_talk.pptx"><button type="button" class="btn btn-outline-primary btn-sm">Slides</button></a>
                <a href="javascript:void(0);"><button type="button" class="btn btn-outline-secondary btn-sm">Talk</button></a>
              </div>
              <div class="collapse" id="e3-abstract">
                <div class="card card-body">
                  Machine learning inference platforms continue to face high request rates and strict latency constraints. 
                  Existing solutions largely focus on compressing models to substantially lower compute costs (and time) with mild accuracy degradations. 
                  This paper explores an alternate (but complementary) technique that trades off accuracy and resource costs on a per-input granularity: 
                  early exit models, which selectively allow certain inputs to exit a model from an intermediate layer. 
                  Though intuitive, early exits face fundamental deployment challenges, largely owing to the effects that exiting inputs have on batch size (and resource utilization) 
                  throughout model execution. We present E3, the first system that makes early exit models practical for realistic inference deployments. 
                  Our key insight is to split and replicate blocks of layers in models in a manner that maintains a constant batch size throughout execution, 
                  all the while accounting for resource requirements and communication overheads. Evaluations with NLP and vision models show that E3 can deliver up to 1.74√ó 
                  improvement in goodput (for a fixed cost) or 1.78√ó reduction in cost (for a fixed goodput). 
                  Additionally, E3's goodput wins generalize to autoregressive LLMs (2.8-3.8√ó) and compressed models (1.67√ó).
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Shockwave: Fair and Efficient Cluster Scheduling for Dynamic Adaptation in Machine Learning</h5> 
              Pengfei Zheng, <b>Rui Pan</b>, Tarannum Khan, Shivaram Venkataraman, Aditya Akella <br>
              <a href="https://www.usenix.org/conference/nsdi23" target="_blank">USENIX NSDI 2023</a>, Boston, MA<br>
              <div class="mt-2">
                <a href="https://www.usenix.org/system/files/nsdi23-zheng.pdf" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#shockwave-abstract" role="button" aria-expanded="false" aria-controls="shockwave-abstract">Abstract</a>
                <a href="publications/nsdi23_shockwave_talk.pdf" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Slides</button></a>
                <a href="https://www.youtube.com/watch?v=d0ZdS12y_Tw" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Talk</button></a>
                <a href="https://github.com/uw-mad-dash/shockwave" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="shockwave-abstract">
                <div class="card card-body">
                  Dynamic adaptation has become an essential technique in accelerating distributed machine learning (ML) training: 
                  Recent studies have shown that dynamically adjusting model structure (e.g., lottery ticket hypothesis) or hyperparameters (e.g., batch size) 
                  can significantly accelerate training without sacrificing accuracy. However, existing ML cluster schedulers are not designed to handle dynamic adaptation. 
                  We show that existing schemes fail to provide fairness and degrade system efficiency when the training throughput changes over time under dynamic adaptation. 
                  We design Shockwave, a scheduler with future planning that builds on two key ideas. 
                  First, Shockwave extends classic market theory from static settings to dynamic settings to co-optimize efficiency and fairness. 
                  Second, Shockwave utilizes stochastic dynamic programming to handle uncertain, dynamic throughput. 
                  We build a system for Shockwave and validate its performance with both trace-driven simulation and cluster experiments. 
                  Results show that for traces of ML jobs with dynamic adaptation, Shockwave improves makespan by 1.3√ó and fairness by 2√ó when compared with existing fair scheduling schemes.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Efficient Flow Scheduling in Distributed Deep Learning Training with Echelon Formation</h5> 
              <b>Rui Pan</b>*, Yiming Lei*, Jialong Li, Zhiqiang Xie, Binhang Yuan, Yiting Xia <br>
              <a href="https://conferences.sigcomm.org/hotnets/2022/" target="_blank">ACM HotNets 2022</a>, Austin, TX<br>
              <div class="mt-2">
                <a href="https://dl.acm.org/doi/10.1145/3563766.3564096?cid=99660641570" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">PDF</button></a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#echelonflow-abstract" role="button" aria-expanded="false" aria-controls="echelonflow-abstract">Abstract</a>
                <a href="https://github.com/ruipeterpan/echelonflow_slides/tree/main" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Slides</button></a>
                <a href="https://youtu.be/kW_7O3cmpIg?si=cSJPEDtCO8q8vLV9&t=976" target="_blank"><button type="button" class="btn btn-outline-primary btn-sm">Talk</button></a>
              </div>
              <div class="collapse" id="echelonflow-abstract">
                <div class="card card-body">
                  This paper discusses why flow scheduling does not apply to distributed deep learning training and presents EchelonFlow, 
                    the first network abstraction to bridge the gap. EchelonFlow deviates from the common belief that semantically 
                    related flows should finish at the same time. We reached the key observation, after extensive workflow analysis 
                    of diverse training paradigms, that distributed training jobs observe strict computation patterns, which may 
                    consume data at different times. We devise a generic method to model the drastically different computation patterns 
                    across training paradigms, and formulate EchelonFlow to regulate flow finish times accordingly. Case studies of 
                    mainstream training paradigms under EchelonFlow demonstrate the expressiveness of the abstraction, and our system 
                    sketch suggests the feasibility of an EchelonFlow scheduling system.
                </div>
              </div>
            </li>
          </ul>
          Some other non peer-reviewed write-ups include:
          <ul>
            <li>CS 759 Project Report: Cautiously Aggressive GPU Space Sharing for Improving Resource Utilization and Job Efficiency (<a href="publications/report_cs759.pdf" target="_blank">pdf</a>)</li>
            <li>CS 744 Project Report: Comparing Black-Box Optimization Methods for Online DBMS Tuning (<a href="publications/report_cs744.pdf" target="_blank">pdf</a>)</li>
            <li>AgDH: A System for Gathering and Disseminating Dairy Data (<a href="publications/wid-poster.pdf" target="_blank">pdf</a>)</li>
          </ul>
        </div>
      </div>    
    </div>  

    <!-- Education -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
          <a id="education" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
          <h2>Education</h2>
          <ul class="list-unstyled">
            <!-- Princeton -->
            <li class="media">
              <img src="images/princeton_square.jpeg" class="mr-3" alt="Princeton University logo" width="64" height="64">
              <div class="media-body">
                <h5 class="mt-0 mb-1">üêÖPrinceton University</h5>
                <p class="badge badge-info badge-dark">Sep 2022 - May 2027?</p>
                <p class="badge badge-info badge-dark">Ph.D. in CS</p>
                <a href="https://www.cs.princeton.edu/~ravian/" target="_blank" class="badge badge-info badge-dark">Advisor: Prof. Ravi Netravali</a>
                <p></p>
              </div>
            </li> 
            <!-- UW-Madison -->
            <li class="media">
              <img src="images/uwmadison.png" class="mr-3" alt="UW Madison logo" width="64" height="64">
              <div class="media-body">
                <h5 class="mt-0 mb-1">ü¶°University of Wisconsin-Madison</h5>
                <p class="badge badge-info badge-dark">Sep 2018 - Dec 2021</p>
                <p class="badge badge-info badge-dark">B.S. in CS & Math</p>
                <!-- <p class="badge badge-info">GPA: 3.96</p> -->
                <a href="https://shivaram.org/" target="_blank" class="badge badge-info badge-dark">Advisor: Prof. Shivaram Venkataraman</a>
                <p></p>
              </div>
            </li> 
          </ul>  
        </div>
      </div>
    </div>

    <!-- Experience -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
          <a id="experience" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
          <h2>Experience</h2>
          <ul class="list-unstyled">
            <li class="media">
              <img src="images/google.jpg" class="mr-3" alt="Google logo" width="64">
              <div class="media-body">
                <h5 class="mt-0 mb-1"><img src="images/PropellerHat.png" height="25"/>Student Researcher @ Google, SystemsResearch@Google</h5>
                <p class="badge badge-info badge-dark">Jun 2025 - Dec 2025</p>
                <p class="badge badge-info badge-dark">Sunnyvale, CA</p>
                <a href="https://sites.google.com/cs.washington.edu/arvind" target="_blank" class="badge badge-info badge-dark">Manager: Prof. Arvind Krishnamurthy</a>
                <p></p>
              </div>
            </li>
            <li class="media">
              <img src="images/aws.jpeg" class="mr-3" alt="AWS logo" width="64">
              <div class="media-body">
                <h5 class="mt-0 mb-1"><img src="images/bezos.png" height="25"/>Applied Scientist Intern @ AWS AI, Machine Learning Systems Team</h5>
                <p class="badge badge-info badge-dark">May 2024 - Dec 2024</p>
                <p class="badge badge-info badge-dark">Santa Clara, CA</p>
                <a href="https://scholar.google.com/citations?user=kHEFQpIAAAAJ&hl=en" target="_blank" class="badge badge-info badge-dark">Manager: Dr. Zhen Jia</a>
                <p></p>
              </div>
            </li>
            <li class="media">
              <img src="images/mpi.jpg" class="mr-3" alt="Max Planck Institute logo" width="64" height="64">
              <div class="media-body">
                <h5 class="mt-0 mb-1">üá©üá™Research Intern @ Max Planck Institute for Informatics (MPI-INF)</h5>
                <p class="badge badge-info badge-dark">Feb 2022 - Aug 2022</p>
                <p class="badge badge-info badge-dark">Saarbr√ºcken, Germany</p>
                <a href="https://sites.google.com/view/yitingxia/home" target="_blank" class="badge badge-info badge-dark">Advisor: Prof. Yiting Xia</a>
                <p></p>
              </div>
            </li>
          </ul>  
        </div>
      </div>
    </div>
    
    <!-- Professional Activities -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h2>Professional Activities</h2>
        </div>
      </div>
      <ul>
        <li>Reviewer: 
          <ul>
            <li>Conferences: <a href="https://mlsys.org/Conferences/2026">MLSys 2026</a>, <a href="https://icml.cc/Conferences/2026">ICML 2026</a></li>
            <li>Workshops: <a href="https://efficient-reasoning.github.io/">NeurIPS 2025 Workshop on Efficient Reasoning</a>, <a href="https://magic-town-d89.notion.site/KDD-2025-Workshop-on-Inference-Optimization-for-Generative-AI-1c8120fc782780b48f7efc32d6de368a">KDD 2025 Workshop on Inference Optimization for Generative AI</a></li>
            <li>Journals: <a href="https://www.jmlr.org/">JMLR</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=71">IEEE TPDS</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=90">IEEE ToN</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6245519">IEEE TCC</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7755">IEEE TMC</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=12">IEEE TC</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9882533">IEEE TMLCN</a>, <a href="https://www.comsoc.org/publications/journals/ieee-comml">IEEE CL</a>, <a href="https://www.sciencedirect.com/journal/computer-networks">CN</a></li>
          </ul>
        </li>
        <li>Artifact Evaluation Committee: MLSys '23, OSDI '23, ATC '23</li>
        <li>Student Volunteer: CMMRS '22, N2Women@SIGCOMM '22</li>
        <li>Teaching Assistant: 
          <ul>
            <li>Fall 2023: <a href="https://cos316.princeton.systems/">COS 316: Principles of Computer System Design</a>, with <a href="http://www.amitlevy.com/" target="_blank">Prof. Amit Levy</a> and <a href="https://www.cs.princeton.edu/~ravian/" target="_blank">Prof. Ravi Netravali</a></li>
            <li>Spring 2024: <a href="https://docs.google.com/document/d/1FIDTU_PIOKMcrHIgUmdF2-LdHfzlYObM/edit" target="_blank">COS 598D: Systems and Machine Learning</a>, with <a href="https://www.cs.princeton.edu/~li/" target="_blank">Prof. Kai Li</a></li>
          </ul>
        </li>
      </ul>
    </div>

    <!-- Interests -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
          <a id="interests" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
          <h2>Interests</h2>
          <ul>
            <li>
              Sports! I play soccer and table tennis for fun and I am a fan of FC Barcelona.
              <!-- I also üö¥ , ü•æ , üèä‚Äç‚ôÇÔ∏è , üé± , ‚õ∏Ô∏è , and wear my Heelys whenever possible. -->
            </li>
            <li>Cooking Chinese dishes!</li>
            <li>Exploring the world through traveling and geoguessing! I love checking out new places, either in person or on Google Maps. Cities I have lived in for more than a few months include: Shanghai, Pittsburgh, Madison, Berkeley, Saarbr√ºcken, Princeton, Mountain View, and Stanford. I play <a href="https://www.geoguessr.com/" target="_blank">GeoGuessr</a> recreationally and have been using <a href="https://fogofworld.app/en/" target="_blank">Fog of World</a> since 2016!</li>
            <!-- <li>Checking out new places, either in person or on Google Maps. Cities I have lived in for more than a few months include: Shanghai, Pittsburgh, Madison, Berkeley, Saarbr√ºcken, Princeton, and Santa Clara.</li> -->
            <li>Watching movies & making pop culture references. One of my favorite Youtube channels is <a href="https://www.youtube.com/@CinemaTherapyShow">Cinema Therapy</a>.</li>
            <!-- <li>Collecting postcards. I love postcards, send me one or let me know if you want one!</li> -->
            <li>Music. I used to play accordion and alto saxophone because of Chinese parenting.</li>
            <!-- Check out my <a href="https://open.spotify.com/user/31g3wuoucmudk6m3u5vhlnh2u2xa" target="_blank">Spotify playlists</a>! -->
            <li>Reading. Some of my recent readings include:</li>
            <ul>
              <li>Nonviolent Communication: A Language of Life (Marshall Rosenberg)</li>
              <li>Eggshell Skull (Bri Lee)</li>
              <li>In the Camps: China's High-Tech Penal Colony (Darren Byler)</li>
            </ul>
            <li>Writing. I have a <a href="https://blog.ruipan.xyz/" target="_blank">personal blog</a> that hosts some paper reading notes and other random blog posts. Some of my most-visited writings include:</li>
            <ul>
              <li><a href="https://blog.ruipan.xyz/blog/towards-applying-to-cs-ph.d.-programs" target="_blank">Towards applying to CS Ph.D. programs</a></li>
              <li><a href="wisc_courses.html" target="_blank">Course Guide @ UW-Madison</a></li>
            </ul>
          </ul>
        </div>
      </div>
    </div>

    <!-- Template -->
    <!-- <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h1>Visitor Map</h1>
        </div>
      </div>
      <div class="row">
        <p></p>
      </div>
    </div> -->

    <!-- Contacts -->
    <div class="jumbotron text-center" style="margin-bottom:0">
      <div class="row">
        <div class="col-sm-12">
          <!-- Fake anchor, otherwise the fixed navbar on the top would cover the title -->
          <a id="contacts" style=" position: relative;top: -60px;display: block;height: 0;overflow: hidden;"></a>
          <h2>Contacts</h2>
          <p>ruipan at princeton dot edu </p>
          <!-- <a href="mailto:ruipan@princeton.edu" class="smlinks"><i class="fas fa-envelope-square fa-2x"></i></a> -->
          <a href="https://www.linkedin.com/in/ruipeterpan/"  class="smlinks" target="_blank"><i class="fab fa-linkedin fa-2x"></i></a>
          <a href="https://github.com/ruipeterpan" class="smlinks" target="_blank"><i class="fab fa-github-square fa-2x"></i></a>
          <a href="https://twitter.com/ruipeterpan" class="smlinks" target="_blank"><i class="fab fa-twitter-square fa-2x"></i></a>
          <a href="https://www.youtube.com/@ruipeterpan" class="smlinks" target="_blank"><i class="fab fa-youtube-square fa-2x"></i></a>
          <a href="https://scholar.google.com/citations?hl=en&user=RPP-4BEAAAAJ" class="smlinks" target="_blank"><i class="ai ai-google-scholar-square ai-2x"></i></a>
          <a href="calendar.html" class="smlinks" target="_blank"><i class="fas fa-calendar-alt fa-2x"></i></i></i></a>
        </div>
      </div>
      <div class="row">
        <div class="col-sm-12">
          <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=000000&w=300&t=n&d=F2TBlxWbs3r0SJixlLgyMRNaIqkp9peGGOY8alVvCD0&co=e9ecef&cmo=f58025'></script>
        </div>
      </div>
      <p>@ 2026 Rui Pan. Powered by Bootstrap. Feel free to fork this website's source code, just remember to remove the analytics stuff and add a reference back to my site.</p>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  </body>
</html>
